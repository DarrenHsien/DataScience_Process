{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# Label Encorder\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_train root :  C:\\Users\\User\\Google 雲端硬碟\\Darren Life Learning Log\\Python_DS\\Dataset\\Data Science London Scikit-learn\\\\train.csv\n",
      "f_trainY root :  C:\\Users\\User\\Google 雲端硬碟\\Darren Life Learning Log\\Python_DS\\Dataset\\Data Science London Scikit-learn\\\\train.csv\n",
      "f_test root :  C:\\Users\\User\\Google 雲端硬碟\\Darren Life Learning Log\\Python_DS\\Dataset\\Data Science London Scikit-learn\\\\test.csv\n"
     ]
    }
   ],
   "source": [
    "#設定路徑\n",
    "dir_data = r\"C:\\Users\\User\\Google 雲端硬碟\\Darren Life Learning Log\\Python_DS\\Dataset\\Data Science London Scikit-learn\\\\\"\n",
    "f_train = os.path.join(dir_data,\"train.csv\")\n",
    "print(\"f_train root : \",f_train)\n",
    "f_trainY = os.path.join(dir_data,\"trainLabels.csv\")\n",
    "print(\"f_trainY root : \",f_train)\n",
    "f_test = os.path.join(dir_data,\"test.csv\")\n",
    "print(\"f_test root : \",f_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape :  (1000, 40)\n",
      "train_labels shape :  (1000, 1)\n",
      "test_data  shape :  (9000, 40)\n"
     ]
    }
   ],
   "source": [
    "train_data  = pd.read_csv(f_train,header=None)\n",
    "print(\"train_data shape : \",train_data .shape)\n",
    "train_labels  = pd.read_csv(f_trainY,header=None)\n",
    "print(\"train_labels shape : \",train_labels .shape)\n",
    "test_data  = pd.read_csv(f_test,header=None)\n",
    "print(\"test_data  shape : \",test_data .shape)\n",
    "#轉np\n",
    "train_data = train_data.values\n",
    "train_labels = train_labels.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料拆分(如果評分標準為CROSS則用不到)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 40), (300, 40), (700,), (300,))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(train_data,train_labels, test_size = 0.30, random_state = 101)\n",
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oringinal Classification\n",
    "### 使用模型\n",
    "- 1.Naive Bayes\n",
    "- 2.KNN 最鄰近分類\n",
    "- 3.LogisticRegression 羅吉斯回歸\n",
    "- 4.SVM 支持向量機\n",
    "- 5.DECISON TREE 決策樹\n",
    "- 6.Random Forest 隨機森林\n",
    "- 7.XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 0.8160000000000001\n",
      "KNN 0.906\n",
      "Logistic Regression 0.8210000000000001\n",
      "SVM 0.915\n",
      "Decision Tree 0.7849999999999999\n",
      "RANDOM Forest 0.8690000000000001\n",
      "XGBoost 0.8690000000000001\n"
     ]
    }
   ],
   "source": [
    "# Naive Bays\n",
    "model = GaussianNB()\n",
    "#model.fit(x_train,y_train.values.ravel())\n",
    "#predicted= model.predict(x_test)\n",
    "#predicted= model.predict(x_test)\n",
    "print('Naive Bayes',cross_val_score(model,train_data,train_labels,cv=10).mean())\n",
    "\n",
    "# KNN\n",
    "knn_model = KNeighborsClassifier()\n",
    "#knn_model.fit(x_train,y_train.values.ravel())\n",
    "#predicted= knn_model.predict(x_test)\n",
    "#print('KNN',accuracy_score(y_test, predicted))\n",
    "print('KNN',cross_val_score(knn_model,train_data,train_labels,cv=10).mean())\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "lr_model = LogisticRegression(solver = 'saga')\n",
    "#lr_model.fit(x_train,y_train.values.ravel())\n",
    "#lr_predicted = lr_model.predict(x_test)\n",
    "#print('Logistic Regression',accuracy_score(y_test, lr_predicted))\n",
    "print('Logistic Regression',cross_val_score(lr_model,train_data,train_labels,cv=10).mean())\n",
    "\n",
    "#SVM\n",
    "svc_model = SVC(gamma = 'auto')\n",
    "#svc_model.fit(x_train,y_train.values.ravel())\n",
    "#svc_predicted = svc_model.predict(x_test)\n",
    "#print('SVM',accuracy_score(y_test, svc_predicted))\n",
    "print('SVM',cross_val_score(svc_model,train_data,train_labels,cv=10).mean())\n",
    "\n",
    "#DECISON TREE\n",
    "dtree_model = DecisionTreeClassifier()\n",
    "#dtree_model.fit(x_train,y_train.values.ravel())\n",
    "#dtree_predicted = dtree_model.predict(x_test)\n",
    "#print('Decision Tree',accuracy_score(y_test, dtree_predicted))\n",
    "print('Decision Tree',cross_val_score(dtree_model,train_data,train_labels,cv=10).mean())\n",
    "\n",
    "# RANDOM FOREST\n",
    "rfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n",
    "#rfc_model.fit(x_train,y_train.values.ravel())\n",
    "#predicted = rfc_model.predict(x_test)\n",
    "#print('Random Forest',accuracy_score(y_test,predicted))\n",
    "print('RANDOM Forest',cross_val_score(rfc_model,train_data,train_labels,cv=10).mean())\n",
    "\n",
    "#XGBOOST\n",
    "xgb = XGBClassifier()\n",
    "#xgb.fit(x_train,y_train.values.ravel())\n",
    "#xgb_predicted = xgb.predict(x_test)\n",
    "#print('XGBoost',accuracy_score(y_test, xgb_predicted))\n",
    "print('XGBoost',cross_val_score(xgb,train_data,train_labels,cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plus 標準化\n",
    "- 1.StandardScaler\n",
    "- 2.Normalizer\n",
    "- 3.MaxMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer()\n",
    "MM = MinMaxScaler()\n",
    "SS = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_data = norm.fit_transform(train_data)\n",
    "mm_train_data = MM.fit_transform(train_data)\n",
    "ss_train_data = SS.fit_transform(train_data)\n",
    "#選用一種標準化資料\n",
    "train_data_norm = norm_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard + Oringinal Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 0.808\n",
      "KNN 0.9019999999999999\n",
      "Logistic Regression 0.8220000000000001\n",
      "SVM 0.808\n",
      "Decision Tree 0.7889999999999999\n",
      "RANDOM Forest 0.8699999999999999\n",
      "XGBoost 0.8710000000000001\n"
     ]
    }
   ],
   "source": [
    "# Naive Bays\n",
    "model = GaussianNB()\n",
    "#model.fit(x_train,y_train.values.ravel())\n",
    "#predicted= model.predict(x_test)\n",
    "#predicted= model.predict(x_test)\n",
    "print('Naive Bayes',cross_val_score(model,train_data_norm,train_labels,cv=10).mean())\n",
    "\n",
    "# KNN\n",
    "knn_model = KNeighborsClassifier()\n",
    "#knn_model.fit(x_train,y_train.values.ravel())\n",
    "#predicted= knn_model.predict(x_test)\n",
    "#print('KNN',accuracy_score(y_test, predicted))\n",
    "print('KNN',cross_val_score(knn_model,train_data_norm,train_labels,cv=10).mean())\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "lr_model = LogisticRegression(solver = 'saga')\n",
    "#lr_model.fit(x_train,y_train.values.ravel())\n",
    "#lr_predicted = lr_model.predict(x_test)\n",
    "#print('Logistic Regression',accuracy_score(y_test, lr_predicted))\n",
    "print('Logistic Regression',cross_val_score(lr_model,train_data_norm,train_labels,cv=10).mean())\n",
    "\n",
    "#SVM\n",
    "svc_model = SVC(gamma = 'auto')\n",
    "#svc_model.fit(x_train,y_train.values.ravel())\n",
    "#svc_predicted = svc_model.predict(x_test)\n",
    "#print('SVM',accuracy_score(y_test, svc_predicted))\n",
    "print('SVM',cross_val_score(svc_model,train_data_norm,train_labels,cv=10).mean())\n",
    "\n",
    "#DECISON TREE\n",
    "dtree_model = DecisionTreeClassifier()\n",
    "#dtree_model.fit(x_train,y_train.values.ravel())\n",
    "#dtree_predicted = dtree_model.predict(x_test)\n",
    "#print('Decision Tree',accuracy_score(y_test, dtree_predicted))\n",
    "print('Decision Tree',cross_val_score(dtree_model,train_data_norm,train_labels,cv=10).mean())\n",
    "\n",
    "# RANDOM FOREST\n",
    "rfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n",
    "#rfc_model.fit(x_train,y_train.values.ravel())\n",
    "#predicted = rfc_model.predict(x_test)\n",
    "#print('Random Forest',accuracy_score(y_test,predicted))\n",
    "print('RANDOM Forest',cross_val_score(rfc_model,train_data_norm,train_labels,cv=10).mean())\n",
    "\n",
    "#XGBOOST\n",
    "xgb = XGBClassifier()\n",
    "#xgb.fit(x_train,y_train.values.ravel())\n",
    "#xgb_predicted = xgb.predict(x_test)\n",
    "#print('XGBoost',accuracy_score(y_test, xgb_predicted))\n",
    "print('XGBoost',cross_val_score(xgb,train_data_norm,train_labels,cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25054403 0.2055048  0.08026473 0.05033658 0.0489595  0.04489903\n",
      " 0.04170779 0.03127928 0.02309806 0.02100116 0.01618423 0.0126823 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca  = PCA(n_components=12)\n",
    "#x_train = pca.fit_transform(x_train)\n",
    "#x_test = pca.transform(x_test)\n",
    "pca_train_data = pca.fit_transform(train_data)\n",
    "explained_variance = pca.explained_variance_ratio_ \n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 12)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 0.842\n",
      "KNN 0.9049999999999999\n",
      "Logistic Regression 0.8230000000000001\n",
      "SVM 0.906\n",
      "Decision Tree 0.796\n",
      "RANDOM Forest 0.909\n",
      "XGBoost 0.8800000000000001\n"
     ]
    }
   ],
   "source": [
    "# Naive Bays\n",
    "model = GaussianNB()\n",
    "#model.fit(x_train,y_train.values.ravel())\n",
    "#predicted= model.predict(x_test)\n",
    "#predicted= model.predict(x_test)\n",
    "print('Naive Bayes',cross_val_score(model,pca_train_data,train_labels,cv=10).mean())\n",
    "\n",
    "# KNN\n",
    "knn_model = KNeighborsClassifier()\n",
    "#knn_model.fit(x_train,y_train.values.ravel())\n",
    "#predicted= knn_model.predict(x_test)\n",
    "#print('KNN',accuracy_score(y_test, predicted))\n",
    "print('KNN',cross_val_score(knn_model,pca_train_data,train_labels,cv=10).mean())\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "lr_model = LogisticRegression(solver = 'saga')\n",
    "#lr_model.fit(x_train,y_train.values.ravel())\n",
    "#lr_predicted = lr_model.predict(x_test)\n",
    "#print('Logistic Regression',accuracy_score(y_test, lr_predicted))\n",
    "print('Logistic Regression',cross_val_score(lr_model,pca_train_data,train_labels,cv=10).mean())\n",
    "\n",
    "#SVM\n",
    "svc_model = SVC(gamma = 'auto')\n",
    "#svc_model.fit(x_train,y_train.values.ravel())\n",
    "#svc_predicted = svc_model.predict(x_test)\n",
    "#print('SVM',accuracy_score(y_test, svc_predicted))\n",
    "print('SVM',cross_val_score(svc_model,pca_train_data,train_labels,cv=10).mean())\n",
    "\n",
    "#DECISON TREE\n",
    "dtree_model = DecisionTreeClassifier()\n",
    "#dtree_model.fit(x_train,y_train.values.ravel())\n",
    "#dtree_predicted = dtree_model.predict(x_test)\n",
    "#print('Decision Tree',accuracy_score(y_test, dtree_predicted))\n",
    "print('Decision Tree',cross_val_score(dtree_model,pca_train_data,train_labels,cv=10).mean())\n",
    "\n",
    "# RANDOM FOREST\n",
    "rfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n",
    "#rfc_model.fit(x_train,y_train.values.ravel())\n",
    "#predicted = rfc_model.predict(x_test)\n",
    "#print('Random Forest',accuracy_score(y_test,predicted))\n",
    "print('RANDOM Forest',cross_val_score(rfc_model,pca_train_data,train_labels,cv=10).mean())\n",
    "\n",
    "#XGBOOST\n",
    "xgb = XGBClassifier()\n",
    "#xgb.fit(x_train,y_train.values.ravel())\n",
    "#xgb_predicted = xgb.predict(x_test)\n",
    "#print('XGBoost',accuracy_score(y_test, xgb_predicted))\n",
    "print('XGBoost',cross_val_score(xgb,pca_train_data,train_labels,cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Gaussian Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_all shape : (10000, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "x_all = np.r_[train_data,test_data]\n",
    "print('x_all shape :',x_all.shape)\n",
    "\n",
    "# USING THE GAUSSIAN MIXTURE MODEL \n",
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "    for n_components in n_components_range:\n",
    "        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n",
    "        gmm.fit(x_all)\n",
    "        bic.append(gmm.aic(x_all))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm\n",
    "            \n",
    "best_gmm.fit(x_all)\n",
    "gmm_train = best_gmm.predict_proba(train_data)\n",
    "gmm_test = best_gmm.predict_proba(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 0.9950000000000001\n",
      "KNN 0.9960000000000001\n",
      "Logistic Regression 0.9960000000000001\n",
      "SVM 0.9960000000000001\n",
      "Decision Tree 0.992\n",
      "RANDOM Forest 0.9960000000000001\n",
      "XGBoost 0.9960000000000001\n"
     ]
    }
   ],
   "source": [
    "# Naive Bays\n",
    "model = GaussianNB()\n",
    "#model.fit(x_train,y_train.values.ravel())\n",
    "#predicted= model.predict(x_test)\n",
    "#predicted= model.predict(x_test)\n",
    "print('Naive Bayes',cross_val_score(model,gmm_train,train_labels,cv=10).mean())\n",
    "\n",
    "# KNN\n",
    "knn_model = KNeighborsClassifier()\n",
    "#knn_model.fit(x_train,y_train.values.ravel())\n",
    "#predicted= knn_model.predict(x_test)\n",
    "#print('KNN',accuracy_score(y_test, predicted))\n",
    "print('KNN',cross_val_score(knn_model,gmm_train,train_labels,cv=10).mean())\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "lr_model = LogisticRegression(solver = 'saga')\n",
    "#lr_model.fit(x_train,y_train.values.ravel())\n",
    "#lr_predicted = lr_model.predict(x_test)\n",
    "#print('Logistic Regression',accuracy_score(y_test, lr_predicted))\n",
    "print('Logistic Regression',cross_val_score(lr_model,gmm_train,train_labels,cv=10).mean())\n",
    "\n",
    "#SVM\n",
    "svc_model = SVC(gamma = 'auto')\n",
    "#svc_model.fit(x_train,y_train.values.ravel())\n",
    "#svc_predicted = svc_model.predict(x_test)\n",
    "#print('SVM',accuracy_score(y_test, svc_predicted))\n",
    "print('SVM',cross_val_score(svc_model,gmm_train,train_labels,cv=10).mean())\n",
    "\n",
    "#DECISON TREE\n",
    "dtree_model = DecisionTreeClassifier()\n",
    "#dtree_model.fit(x_train,y_train.values.ravel())\n",
    "#dtree_predicted = dtree_model.predict(x_test)\n",
    "#print('Decision Tree',accuracy_score(y_test, dtree_predicted))\n",
    "print('Decision Tree',cross_val_score(dtree_model,gmm_train,train_labels,cv=10).mean())\n",
    "\n",
    "# RANDOM FOREST\n",
    "rfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n",
    "#rfc_model.fit(x_train,y_train.values.ravel())\n",
    "#predicted = rfc_model.predict(x_test)\n",
    "#print('Random Forest',accuracy_score(y_test,predicted))\n",
    "print('RANDOM Forest',cross_val_score(rfc_model,gmm_train,train_labels,cv=10).mean())\n",
    "\n",
    "#XGBOOST\n",
    "xgb = XGBClassifier()\n",
    "#xgb.fit(x_train,y_train.values.ravel())\n",
    "#xgb_predicted = xgb.predict(x_test)\n",
    "#print('XGBoost',accuracy_score(y_test, xgb_predicted))\n",
    "print('XGBoost',cross_val_score(xgb,gmm_train,train_labels,cv=10).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上傳kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTree 0.998\n"
     ]
    }
   ],
   "source": [
    "rfc_model.fit(gmm_train,train_labels)\n",
    "rfc_predicted = dtree_model.predict(gmm_train)\n",
    "print('DTree',accuracy_score(train_labels, rfc_predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pr = pd.DataFrame(dtree_model.predict(gmm_test))\n",
    "rfc_pr.columns = ['Solution',]\n",
    "rfc_pr['Id'] = np.arange(1,rfc_pr.shape[0]+1)\n",
    "rfc_pr = rfc_pr[['Id','Solution']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pr.to_csv('Submission_GMM_RFC.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
